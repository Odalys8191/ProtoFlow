import os
import json
import argparse
import itertools
import numpy as np
import sys
import time

# å¯¼å…¥trainæ¨¡å—
import train

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================

# SOTA JSON è·¯å¾„
SOTA_PATH = "../Data/sota.json"
# ç»“æœä¿å­˜æ ¹ç›®å½•
RESULT_ROOT = "result"

# æŒ‡æ ‡åç§°æ˜ å°„ï¼štrain.pyè¿”å›çš„æŒ‡æ ‡å -> è¾“å‡ºçš„æŒ‡æ ‡å
METRICS_MAPPING = {
    'cheby': 'Chebyshev',
    'clark': 'Clark',
    'canberra': 'Canberra',
    'kl_div': 'KL Divergence',
    'cosine': 'Cosine',
    'intersection': 'Intersection'
}

# æ˜ å°„å…³ç³»ï¼šæ–‡ä»¶å¤¹å -> SOTA JSON é‡Œçš„ Key
DATASET_MAPPING = {
    "Gene": "Gene",
    "Movie": "Movie",
    "RAF_ML": "RAF_ML",
    "Ren_Cecps": "Ren_Cecps",
    "SBU_3DFE": "SBU_3DFE",
    "Scene": "Scene",
    
    "Flickr_LDL": "Flickr_LDL",
    "M2B": "M2B",
    "SCUT_FBP": "SCUT_FBP",
    "SCUT_FBP5500": "SCUT_FBP5500",
    "SJAFFE": "SJAFFE",
    "Twitter_LDL": "Twitter_LDL"
}

# Grid Search æœç´¢ç©ºé—´
SEARCH_SPACE = {
    "lr": [1e-3, 5e-4, 1e-4, 1e-5],
    "batch_size": [128, 256],
    "feature_dim": [64, 128, 256, 512]
}

# æŒ‡æ ‡åç§°
METRICS_NAMES = ['Chebyshev', 'Clark', 'Canberra', 'KL Divergence', 'Cosine', 'Intersection']
SOTA_KEYS = ['Cheby', 'Clark', 'Canbe', 'KL', 'Cosine', 'Inter']

# ================= åŠŸèƒ½å‡½æ•° =================

def get_sota_directly(dataset_folder_name):
    """ç›´æ¥è¯»å– JSON å†…å®¹è·å–SOTAæŒ‡æ ‡"""
    if not os.path.exists(SOTA_PATH):
        print(f"âŒ Error: {SOTA_PATH} not found.")
        return None

    try:
        with open(SOTA_PATH, 'r', encoding='utf-8') as f:
            full_data = json.load(f)
        
        data_block = full_data.get('data', {})
        # é€šè¿‡æ˜ å°„è¡¨æ‰¾åˆ° JSON é‡Œçš„ key
        target_key = DATASET_MAPPING.get(dataset_folder_name, dataset_folder_name)
        
        if target_key not in data_block:
            print(f"âš ï¸ SOTA data for '{target_key}' not found (Folder: {dataset_folder_name}).")
            return None
        
        vals = []
        for key in SOTA_KEYS:
            vals.append(data_block[target_key][key]['mean'])
        
        print(f"ğŸ“š Loaded SOTA for {dataset_folder_name}: {vals}")
        return vals

    except Exception as e:
        print(f"âŒ JSON Error: {e}")
        return None


def calc_avg_imp(our_mean, sota_vals):
    """è®¡ç®—å¹³å‡æ”¹è¿›ç‡"""
    if not sota_vals:
        return 0.0
    imps = []
    for i in range(4): # å‰4ä¸ªæŒ‡æ ‡è¶Šå°è¶Šå¥½
        imps.append((sota_vals[i] - our_mean[i]) / (sota_vals[i] + 1e-12))
    for i in range(4, 6): # å2ä¸ªæŒ‡æ ‡è¶Šå¤§è¶Šå¥½
        imps.append((our_mean[i] - sota_vals[i]) / (sota_vals[i] + 1e-12))
    return np.mean(imps)


def get_run_files(dataset_path, run_idx):
    """è·å–æŒ‡å®šrunçš„æ–‡ä»¶è·¯å¾„"""
    run_dir = os.path.join(dataset_path, f"run_{run_idx}")
    return {
        "train_feature": os.path.join(run_dir, "train_feature.npy"),
        "train_label": os.path.join(run_dir, "train_label.npy"),
        "test_feature": os.path.join(run_dir, "test_feature.npy"),
        "test_label": os.path.join(run_dir, "test_label.npy")
    }

# ================= ä¸»é€»è¾‘ =================

def main(dataset, device):
    # 1. å‡†å¤‡ç›®å½•
    save_dir = os.path.join(RESULT_ROOT, dataset)
    os.makedirs(save_dir, exist_ok=True)
    txt_path = os.path.join(save_dir, "result.txt")
    best_params_path = os.path.join(save_dir, "best_params.json")
    best_model_path = os.path.join(save_dir, "best_model_path.txt")

    print(f"ğŸš€ Processing Dataset: {dataset} on {device}")
    
    # 2. è·å– SOTA
    sota_vals = get_sota_directly(dataset)

    # 3. Grid Search (åœ¨ run_0 ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯)
    print(f"\n{'='*30}\nğŸ” Grid Search (run_0 with 5-fold CV)\n{'='*30}")
    
    keys, values = zip(*SEARCH_SPACE.items())
    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]
    
    best_avg_imp = -float('inf')
    best_params = {}
    best_cv_metrics = None

    for params in combinations:
        print(f"\nTesting Params: {params}")
        
        # è·å–run_0çš„æ–‡ä»¶è·¯å¾„
        dataset_path = os.path.join("/home/ubuntu/zxj/Data/feature", dataset)
        run_0_files = get_run_files(dataset_path, 0)
        
        # ç›´æ¥è°ƒç”¨trainæ¨¡å—çš„runå‡½æ•°ï¼Œä½¿ç”¨create_train_argsåˆ›å»ºå‚æ•°
        train_args = train.create_train_args(
            train_feature=run_0_files["train_feature"],
            train_label=run_0_files["train_label"],
            test_feature=run_0_files["test_feature"],
            test_label=run_0_files["test_label"],
            device=device,
            num_epochs=200,
            num_workers=0,
            cv_folds=5,  # äº”æŠ˜äº¤å‰éªŒè¯
            extra=f"grid_search_{dataset}",
            **params  # ä¼ é€’æ‰€æœ‰è¶…å‚æ•°
        )
        
        # è°ƒç”¨train.runå‡½æ•°è·å–äº¤å‰éªŒè¯ç»“æœ
        cv_results = train.run(train_args)
        
        if cv_results:
            # å°†cv_resultsè½¬æ¢ä¸ºä¸METRICS_NAMESå¯¹åº”çš„åˆ—è¡¨
            metrics = [cv_results['cheby'], cv_results['clark'], cv_results['canberra'], 
                      cv_results['kl_div'], cv_results['cosine'], cv_results['intersection']]
            avg_imp = calc_avg_imp(metrics, sota_vals)
            print(f"ğŸ‘‰ Result Metrics: {metrics}")
            print(f"ğŸ‘‰ AvgImp: {avg_imp:.2%}")
            
            if avg_imp > best_avg_imp:
                best_avg_imp = avg_imp
                best_params = params
                best_cv_metrics = metrics
                print("â­ Current Best!")
        else:
            print("âŒ Failed to get cv results")

    print(f"\nâœ… Best Params Found: {best_params} (AvgImp: {best_avg_imp:.2%})")
    
    # ä¿å­˜æœ€ä¼˜å‚æ•°
    with open(best_params_path, 'w', encoding='utf-8') as f:
        json.dump(best_params, f, indent=2)
    print(f"ğŸ’¾ Saved best_params.json to {best_params_path}")

    # 4. è·‘æ‰€æœ‰ run_0 è‡³ run_9
    print(f"\n{'='*30}\nğŸƒ Running 10 runs (run_0 to run_9)\n{'='*30}")
    
    all_run_metrics = []
    best_model_paths = []

    for run_idx in range(10):
        print(f"\n>>> Run {run_idx}/9")
        
        # è·å–å½“å‰runçš„æ–‡ä»¶è·¯å¾„
        run_files = get_run_files(dataset_path, run_idx)
        
        # åˆ›å»ºè®­ç»ƒå‚æ•°
        train_args = train.create_train_args(
            train_feature=run_files["train_feature"],
            train_label=run_files["train_label"],
            test_feature=run_files["test_feature"],
            test_label=run_files["test_label"],
            device=device,
            batch_size=best_params["batch_size"],
            lr=best_params["lr"],
            num_epochs=200,
            num_workers=0,
            cv_folds=1,  # éäº¤å‰éªŒè¯æ¨¡å¼
            extra=f"{dataset}_run{run_idx}",
            **best_params  # ä¼ é€’å…¶ä»–è¶…å‚æ•°
        )
        
        print(f"\nğŸ‹ï¸ Training on run {run_idx}...")
        # è°ƒç”¨train.runå‡½æ•°è·å–æµ‹è¯•ç»“æœ
        test_results = train.run(train_args)
        
        if test_results:
            # å°†test_resultsè½¬æ¢ä¸ºä¸METRICS_NAMESå¯¹åº”çš„åˆ—è¡¨
            metrics = [test_results['cheby'], test_results['clark'], test_results['canberra'], 
                      test_results['kl_div'], test_results['cosine'], test_results['intersection']]
            all_run_metrics.append(metrics)
            print(f"âœ… Run {run_idx} Metrics: {metrics}")
            
            # ä¿å­˜å½“å‰runçš„æœ€ä½³æ¨¡å‹è·¯å¾„
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾train.pyä¼šåœ¨logsç›®å½•ä¸‹ç”Ÿæˆæ¨¡å‹æ–‡ä»¶
            log_dir = os.path.join("logs", f"ldl_{dataset}_run{run_idx}")
            if os.path.exists(log_dir):
                for root, dirs, files in os.walk(log_dir):
                    for file in files:
                        if file.endswith(".pt") and "best" in file:
                            model_path = os.path.abspath(os.path.join(root, file))
                            best_model_paths.append(model_path)
                            print(f"ğŸ“¦ Saved model checkpoint: {model_path}")
                            break
        else:
            print(f"âŒ Failed to get metrics for run {run_idx}")

    # 5. å­˜æ¡£å’Œç»“æœè¾“å‡º
    if not all_run_metrics:
        print("âŒ No results.")
        return

    all_run_metrics = np.array(all_run_metrics)
    means = np.mean(all_run_metrics, axis=0)
    stds = np.std(all_run_metrics, axis=0)
    overall_avg_imp = calc_avg_imp(means, sota_vals)

    lines = []
    lines.append(f"Dataset: {dataset}")
    lines.append(f"Best Params: {best_params}")
    lines.append(f"Best Model Checkpoints:")
    for i, path in enumerate(best_model_paths):
        lines.append(f"  Run {i}: {path}")
    lines.append("-" * 85)
    lines.append(f"{'Metric':<15} | {'Mean Â± Std':<25} | {'SOTA':<10}")
    lines.append("-" * 85)
    
    for i, name in enumerate(METRICS_NAMES):
        sota_str = f"{sota_vals[i]:.3f}" if sota_vals else "N/A"
        lines.append(f"{name:<15} | {means[i]:.4f} Â± {stds[i]:.4f} | {sota_str:<10}")
        
    lines.append("-" * 85)
    lines.append(f"Overall AvgImp: {overall_avg_imp:.2%}")
    lines.append("-" * 85)
    lines.append("Runs Results:")
    for run_idx, metrics in enumerate(all_run_metrics):
        lines.append(f"  Run {run_idx}: {metrics}")
    
    final_content = "\n".join(lines)
    print("\n" + final_content)

    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(final_content)
    print(f"\nğŸ’¾ Saved result.txt to {txt_path}")
    
    # ä¿å­˜æœ€ä¼˜æ¨¡å‹checkpointè·¯å¾„
    if best_model_paths:
        with open(best_model_path, 'w', encoding='utf-8') as f:
            for path in best_model_paths:
                f.write(f"{path}\n")
        print(f"ğŸ’¾ Saved best_model_path.txt to {best_model_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", required=True, help="Dataset name")
    parser.add_argument("--device", required=True, help="Device ID, e.g., 'cuda:0'")
    args = parser.parse_args()
    main(args.dataset, args.device)
