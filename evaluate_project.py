import os
import os.path as osp
import json
import argparse
import subprocess
import itertools
import numpy as np
import re
import sys
import time

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================

# SOTA JSON è·¯å¾„
SOTA_PATH = "../Data/sota.json"
# ç»“æœä¿å­˜æ ¹ç›®å½•
RESULT_ROOT = "evaluation_results"
# è®­ç»ƒè„šæœ¬å
TRAIN_SCRIPT = "train.py"
# æ¨ç†è„šæœ¬åï¼ˆå¦‚æœä¸è®­ç»ƒè„šæœ¬åˆ†ç¦»ï¼‰
INFERENCE_SCRIPT = "test.py"

# æ˜ å°„å…³ç³»ï¼šæ–‡ä»¶å¤¹å -> SOTA JSON é‡Œçš„ Key
DATASET_MAPPING = {
    "Gene": "Gene",
    "Movie": "Movie",
    "RAF_ML": "RAF_ML",
    "Ren_Cecps": "Ren_Cecps",
    "SBU_3DFE": "SBU_3DFE",
    "Scene": "Scene",
    
    "Flickr_LDL": "Flickr_LDL",
    "M2B": "M2B",
    "SCUT_FBP": "SCUT_FBP",
    "SCUT_FBP5500": "SCUT_FBP5500",
    "SJAFFE": "SJAFFE",
    "Twitter_LDL": "Twitter_LDL"
}

# Grid Search æœç´¢ç©ºé—´
SEARCH_SPACE = {
    "lr": [1e-3, 5e-4, 1e-4, 1e-5],
    "batch_size": [128, 256],
    "feature_dim": [64, 128, 256, 512]
}

# æŒ‡æ ‡åç§°
METRICS_NAMES = ['Chebyshev', 'Clark', 'Canberra', 'KL Divergence', 'Cosine', 'Intersection']
SOTA_KEYS = ['Cheby', 'Clark', 'Canbe', 'KL', 'Cosine', 'Inter']

# äº¤å‰éªŒè¯æŠ˜æ•°
CV_FOLDS = 5
# æ¯æ¬¡æ¨ç†çš„é‡å¤æ¬¡æ•°
INFERENCE_REPEATS = 10

# ================= åŠŸèƒ½å‡½æ•° =================


def get_sota_directly(dataset_folder_name):
    """ç›´æ¥è¯»å– JSON å†…å®¹"""
    if not os.path.exists(SOTA_PATH):
        print(f"âŒ Error: {SOTA_PATH} not found.")
        return None

    try:
        with open(SOTA_PATH, 'r', encoding='utf-8') as f:
            full_data = json.load(f)
        
        data_block = full_data.get('data', {})
        # é€šè¿‡æ˜ å°„è¡¨æ‰¾åˆ° JSON é‡Œçš„ key
        target_key = DATASET_MAPPING.get(dataset_folder_name, dataset_folder_name)
        
        if target_key not in data_block:
            print(f"âš ï¸ SOTA data for '{target_key}' not found (Folder: {dataset_folder_name}).")
            return None
        
        vals = []
        for key in SOTA_KEYS:
            vals.append(data_block[target_key][key]['mean'])
        
        print(f"ğŸ“š Loaded SOTA for {dataset_folder_name}: {vals}")
        return vals

    except Exception as e:
        print(f"âŒ JSON Error: {e}")
        return None



def parse_metrics(output_str):
    results = []
    for m in METRICS_NAMES:
        pattern = re.escape(m) + r"\s+\|\s+([0-9\.]+)"
        match = re.search(pattern, output_str)
        if match:
            results.append(float(match.group(1)))
    return results if len(results) == 6 else None



def calc_avg_imp(our_mean, sota_vals):
    if not sota_vals: return 0.0
    imps = []
    for i in range(4): # å‰4ä¸ªè¶Šå°è¶Šå¥½
        imps.append((sota_vals[i] - our_mean[i]) / (sota_vals[i] + 1e-12))
    for i in range(4, 6): # å2ä¸ªè¶Šå¤§è¶Šå¥½
        imps.append((our_mean[i] - sota_vals[i]) / (sota_vals[i] + 1e-12))
    return np.mean(imps)



def run_cmd_live(cmd, repeats=1):
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœï¼Œæ”¯æŒå¤šæ¬¡è¿è¡Œå–å¹³å‡"""
    all_metrics = []
    full_output = ""
    
    for i in range(repeats):
        print(f"\nğŸ“Š Run {i+1}/{repeats}")
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            if line:
                sys.stdout.write(line)
                full_output += line
        
        # ä»å®Œæ•´è¾“å‡ºä¸­è§£æmetrics
        metrics = parse_metrics(full_output)
        if metrics:
            all_metrics.append(metrics)
        else:
            print(f"âŒ Failed to parse metrics for run {i+1}")
    
    if not all_metrics:
        return None, full_output
    
    avg_metrics = np.mean(all_metrics, axis=0).tolist()
    return avg_metrics, full_output


# ================= ä¸»é€»è¾‘ =================

def main(dataset, device):
    # 1. å‡†å¤‡ç›®å½•
    save_dir = os.path.join(RESULT_ROOT, dataset)
    os.makedirs(save_dir, exist_ok=True)
    model_dir = os.path.join(save_dir, "model")
    os.makedirs(model_dir, exist_ok=True)
    txt_path = os.path.join(save_dir, "result.txt")
    best_params_path = os.path.join(save_dir, "best_params.json")

    print(f"ğŸš€ Processing Dataset: {dataset} on {device}")
    
    # 2. è·å– SOTA
    sota_vals = get_sota_directly(dataset)

    # 3. Grid Search (åœ¨ run_0 ä¸‹æµ‹è¯•è¶…å‚æ•°ï¼Œç”¨äº”æŠ˜äº¤å‰éªŒè¯é‡å¤äº”æ¬¡å’Œ avg_imp ä½œä¸ºä¼˜å…ˆåº¦)
    print(f"\n{'='*30}\nğŸ” Grid Search (run_0 with 5-fold CV repeated 5 times)\n{'='*30}")
    
    keys, values = zip(*SEARCH_SPACE.items())
    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]
    
    best_avg_imp = -float('inf')
    best_params = {}

    for params in combinations:
        print(f"\nTesting Params: {params}")
        
        # æ„é€ è®­ç»ƒå‘½ä»¤ï¼šæ ¹æ®å®é™…é¡¹ç›®è°ƒæ•´å‚æ•°
        train_cmd = [
            "python", TRAIN_SCRIPT,
            "--train_feature", f"../Data/{dataset}/feature/train_feature.npy",
            "--train_label", f"../Data/{dataset}/label/train_label.npy",
            "--test_feature", f"../Data/{dataset}/feature/test_feature.npy",
            "--test_label", f"../Data/{dataset}/label/test_label.npy",
            "--batch_size", str(params.get("batch_size", 128)),
            "--lr", str(params.get("lr", 1e-3)),
            "--feature_dim", str(params.get("feature_dim", 128)),
            "--nepoch", "200",
            "--num_workers", "0",
            "--extra", f"grid_search_{dataset}",
            "--cv_folds", "5"  # æŒ‡å®šäº”æŠ˜äº¤å‰éªŒè¯
        ]
        
        print(f"\nğŸ‹ï¸  Training with params: {params}")
        # è¿è¡Œè®­ç»ƒï¼Œä¿å­˜æ¨¡å‹
        _, train_output = run_cmd_live(train_cmd, repeats=1)
        
        # å¯»æ‰¾è®­ç»ƒç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶
        model_files = []
        grid_search_dir = osp.join("logs", f"ldl_grid_search_{dataset}")
        if os.path.exists(grid_search_dir):
            for f in os.listdir(grid_search_dir):
                if f.startswith("bs"):
                    model_run_dir = osp.join(grid_search_dir, f)
                    for mf in os.listdir(model_run_dir):
                        if mf.endswith(".pt") and "best" in mf:
                            model_files.append(osp.join(model_run_dir, mf))
        
        if model_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„æ¨¡å‹
            latest_model = max(model_files, key=os.path.getmtime)
            print(f"ğŸ“¦ Using latest model: {latest_model}")
            
            # æ„é€ æ¨ç†å‘½ä»¤
            inference_cmd = [
                "python", INFERENCE_SCRIPT,
                "--resume", latest_model,
                "--train_feature", f"../Data/{dataset}/feature/train_feature.npy",
                "--train_label", f"../Data/{dataset}/label/train_label.npy",
                "--test_feature", f"../Data/{dataset}/feature/test_feature.npy",
                "--test_label", f"../Data/{dataset}/label/test_label.npy",
                "--batch_size", "2048",
                "--num_samples", "10",  # è·‘10æ¬¡inference
                "--test_only"
            ]
            
            # äº”æŠ˜äº¤å‰éªŒè¯é‡å¤äº”æ¬¡ï¼šæ¯æ¬¡inferenceè·‘10æ¬¡å–å¹³å‡
            print(f"\nğŸ”„ 5-fold CV repeated 5 times for params: {params}")
            all_cv_metrics = []
            for cv_repeat in range(5):
                print(f"ğŸ“Š CV Repeat {cv_repeat+1}/5")
                for fold_idx in range(5):
                    print(f"  Fold {fold_idx+1}/5")
                    # æ¯æ¬¡inferenceä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼Œè·‘10æ¬¡å–å¹³å‡
                    fold_metrics, _ = run_cmd_live(inference_cmd, repeats=1)
                    if fold_metrics:
                        all_cv_metrics.append(fold_metrics)
            
            if all_cv_metrics:
                # è®¡ç®—äº”æŠ˜é‡å¤äº”æ¬¡çš„å¹³å‡metrics
                cv_avg_metrics = np.mean(all_cv_metrics, axis=0).tolist()
                # è®¡ç®—avg_impä½œä¸ºä¼˜å…ˆåº¦æŒ‡æ ‡
                cv_avg_imp = calc_avg_imp(cv_avg_metrics, sota_vals)
                print(f"\nğŸ“Š 5-fold CV repeated 5 times Results for {params}:")
                print(f"   Mean Metrics: {cv_avg_metrics}")
                print(f"   AvgImp: {cv_avg_imp:.2%}")
                
                # é€‰æ‹©avg_impæœ€é«˜çš„å‚æ•°
                if cv_avg_imp > best_avg_imp:
                    best_avg_imp = cv_avg_imp
                    best_params = params
                    print("â­ Current Best!")
            else:
                print("âŒ Failed to get metrics for all folds")
        else:
            print("âŒ No model files found after training")

    print(f"\nâœ… Best Params Found: {best_params} (AvgImp: {best_avg_imp:.2%})")
    
    # ä¿å­˜æœ€ä¼˜å‚æ•°
    with open(best_params_path, 'w', encoding='utf-8') as f:
        json.dump(best_params, f, indent=2)
    print(f"ğŸ’¾ Saved best_params.json to {best_params_path}")

    # 4. è·‘æ‰€æœ‰ run_0 è‡³ run_9ï¼Œæ¯ä¸ª run è·‘åæ¬¡ inference å–å¹³å‡
    print(f"\n{'='*30}\nğŸƒ Running all runs (run_0 to run_9)\n{'='*30}")
    
    all_run_metrics = []
    for run_idx in range(10):
        print(f"\n>>> Run {run_idx}/9")
        
        # æ„é€ è®­ç»ƒå‘½ä»¤ï¼šä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
        # æ³¨æ„ï¼štrain.pyéœ€è¦æŒ‡å®šç‰¹å¾å’Œæ ‡ç­¾æ–‡ä»¶è·¯å¾„ï¼Œè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®é›†ç»“æ„è°ƒæ•´
        train_feature_path = f"../Data/{dataset}/feature/train_feature.npy"
        train_label_path = f"../Data/{dataset}/label/train_label.npy"
        test_feature_path = f"../Data/{dataset}/feature/test_feature.npy"
        test_label_path = f"../Data/{dataset}/label/test_label.npy"
        
        train_cmd = [
            "python", TRAIN_SCRIPT,
            "--train_feature", train_feature_path,
            "--train_label", train_label_path,
            "--test_feature", test_feature_path,
            "--test_label", test_label_path,
            "--batch_size", str(best_params.get("batch_size", 128)),
            "--lr", str(best_params.get("lr", 1e-3)),
            "--feature_dim", str(best_params.get("feature_dim", 128)),
            "--nepoch", "200",
            "--num_workers", "0",
            "--device", device,
            "--extra", f"{dataset}_run{run_idx}"
        ]
        
        print(f"\nğŸ‹ï¸ Training with best params for run {run_idx}...")
        train_output = run_cmd_live(train_cmd, repeats=1)[1]
        
        # æ„é€ æ¨ç†å‘½ä»¤ï¼šåŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè·‘åæ¬¡å–å¹³å‡
        # å¯»æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
        model_files = []
        for f in os.listdir(osp.join("logs", f"ldl_{dataset}_run{run_idx}")):
            if f.startswith("bs"):
                model_run_dir = osp.join("logs", f"ldl_{dataset}_run{run_idx}", f)
                for mf in os.listdir(model_run_dir):
                    if mf.endswith(".pt") and "best" in mf:
                        model_files.append(osp.join(model_run_dir, mf))
        
        if model_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„æ¨¡å‹
            latest_model = max(model_files, key=os.path.getmtime)
            print(f"ğŸ“¦ Using latest model: {latest_model}")
            
            inference_cmd = [
                    "python", INFERENCE_SCRIPT,
                    "--resume", latest_model,
                    "--train_feature", train_feature_path,
                    "--train_label", train_label_path,
                    "--test_feature", test_feature_path,
                    "--test_label", test_label_path,
                    "--batch_size", "2048",
                    "--num_samples", "10",  # è·‘10æ¬¡inference
                    "--test_only"
                ]
            
            print(f"\nğŸ” Inferencing with best model for run {run_idx}...")
            metrics, _ = run_cmd_live(inference_cmd, repeats=INFERENCE_REPEATS)
        else:
            print(f"âŒ No best model found for run {run_idx}")
            metrics = None
        
        if metrics:
            all_run_metrics.append(metrics)
            print(f"âœ… Run {run_idx} Metrics (avg of {INFERENCE_REPEATS} inferences): {metrics}")
        else:
            print(f"âŒ Failed to get metrics for run {run_idx}")

    # 5. è®¡ç®—æœ€ç»ˆç»“æœ
    if not all_run_metrics:
        print("âŒ No valid results from any run.")
        return

    all_run_metrics = np.array(all_run_metrics)
    means = np.mean(all_run_metrics, axis=0)
    stds = np.std(all_run_metrics, axis=0)
    overall_avg_imp = calc_avg_imp(means, sota_vals)

    # 6. ç”Ÿæˆå¹¶ä¿å­˜ç»“æœæŠ¥å‘Š
    lines = []
    lines.append(f"Dataset: {dataset}")
    lines.append(f"Best Params: {best_params}")
    lines.append("-" * 65)
    lines.append(f"{'Metric':<15} | {'Mean Â± Std':<25} | {'SOTA':<10}")
    lines.append("-" * 65)
    
    for i, name in enumerate(METRICS_NAMES):
        sota_str = f"{sota_vals[i]:.3f}" if sota_vals else "N/A"
        lines.append(f"{name:<15} | {means[i]:.4f} Â± {stds[i]:.4f} | {sota_str:<10}")
        
    lines.append("-" * 65)
    lines.append(f"Overall AvgImp: {overall_avg_imp:.2%}")
    lines.append("-" * 65)
    lines.append("Runs Results:")
    for run_idx, metrics in enumerate(all_run_metrics):
        lines.append(f"  Run {run_idx}: {metrics}")
    
    final_content = "\n".join(lines)
    print("\n" + final_content)

    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(final_content)
    print(f"\nğŸ’¾ Saved result.txt to {txt_path}")
    print(f"\nğŸ‰ Evaluation completed successfully for {dataset}!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", required=True, help="Dataset name")
    parser.add_argument("--device", required=True, help="Device ID, e.g., 'cuda:0'")
    args = parser.parse_args()
    main(args.dataset, args.device)
